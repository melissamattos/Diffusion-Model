# Diffusion Model for MNIST

### Overview

This repository contains the implementation of a diffusion model for generating images from the MNIST dataset. The model uses an approach based on Denoising Diffusion Probabilistic Models (DDPM) and includes functions for training, inference, and visualization of results.

### Why Use a Diffusion Model Instead of BiGAN?

In the original WellGT architecture, a Bidirectional Generative Adversarial Network (BiGAN) was used to generate negative samples for the similarity learning task. While BiGAN provided a mechanism to create synthetic well-log data, it had limitations:

- **Training Instability**: GAN-based models, including BiGAN, often suffer from mode collapse and training instability, requiring careful hyperparameter tuning.

- **Lack of Diversity in Negative Samples**: The negative samples generated by BiGAN might not be sufficiently diverse, potentially affecting the quality of similarity learning.

- **Difficulty in Matching Real Data Distribution**: Generative adversarial models struggle to accurately model high-dimensional, structured data like well-log sequences.

### Advantages of Using a Diffusion Model

By replacing BiGAN with a diffusion model, we can overcome these limitations:

- **More Stable Training**: DDPM models train using a likelihood-based approach rather than adversarial loss, avoiding the instability of GANs.

- **Better Negative Sampling**: Diffusion models can generate high-quality, realistic perturbations of well-log sequences, improving the contrastive learning process.

- **Improved Similarity Encoding**: By leveraging the structured noise scheduling of diffusion models, the embeddings learned are more robust, leading to better well-log similarity assessments.

## Project Structure

The project consists of the following main files:

- `config.py`: Global project configurations, including hyperparameters, image size, and device settings (GPU or CPU).
- `ddpm.py`: Implementation of the diffusion model, including the denoising network based on a simplified U-Net and auxiliary functions for perturbation and image recovery.
- `main.py`: Main script that performs training and generates images from the trained model.
- `mnist_dataloader.py`: Code responsible for loading the MNIST dataset and preparing it for training.
- `DDPM_cosine.ipynb`: Notebook for experimentation and execution of the diffusion model with cosine-based noise scheduling.
- `how_to_use.ipynb`: Explanatory notebook on how to use the scripts to train and test the model.

Make sure you have the following folder structure in the data directory:

```bash
/your_project/
│── main.py                  
│── config.py  
|── requirements.txt
│── models/
│   ├── __init__.py          
│   ├── ddpm.py              
│── data/
│   ├── __init__.py          
│   ├── mnist_dataloader.py  
│── checkpoints/
│── generated_images/
│   ├── Generated Images.png          
│   ├── Perturbed Images.png
│── DDPM_cosine.ipynb
│── how_to_use.ipynb
```

### Key Files
- `main.py` – Main script for training the model and generating images.
- `config.py` – Defines global configurations, including hyperparameters, image size, and device settings (GPU/CPU).
- `requirements.txt` – List of dependencies required for the project to run properly.
- `ddpm.py` – Implements the diffusion model with a simplified U-Net and functions for noise perturbation and denoising.
- `mnist_dataloader.py` – Handles dataset loading and preprocessing.
- `DDPM_cosine.ipynb` – Jupyter Notebook for experimentation with cosine-based noise scheduling.
- `how_to_use.ipynb` – Tutorial on training and evaluating the model.

## Installation

To run this project, you need to have Python 3.10. Install all the depedencies required by executing the following command within the main project folder:

```bash
pip install -r requirements.txt
```

## Usage

### Configuration

Before running the training script, you need to set the paths for your training and test datasets. These paths should be configured in the `config.py` file.

#### How to Set Dataset Paths

- Locate the `config.py` file in the root directory of the project.
- Edit the values for `train_data_path` and `test_data_path` with the actual file paths where your datasets are stored.
- Save the file.

The model's hyperparameters can be adjusted in the `config.py` file, including:

- `img_size`: Image size (28x28 for MNIST).
- `timestep_embedding_dim`: Dimension of the time embedding.
- `n_layers`: Number of layers in the neural network.
- `n_timesteps`: Number of diffusion steps.
- `train_batch_size`: Batch size for training.
- `lr`: Learning rate.
- `epochs`: Number of training epochs.

Ensure that you have installed all dependencies before running the script. For installation instructions, refer to the `requirements.txt` file.

### Training the Model

Once the dataset paths are configured, run the training script with the following command:

```bash
python3 main.py
```

This will load the MNIST dataset, initialize the model, and start the training process. During training, model checkpoints will be automatically saved in the `checkpoints/` directory. These checkpoints allow you to resume training from the last saved state or use a pre-trained model for inference.

### Generated Images

The images will be stored in the `generated_images/` folder. Ensure this directory exists before running inference, or the script will create it automatically. The generated images can be used for evaluation and visualization of the model's performance.

## Visualizing Results

The notebooks `DDPM_cosine.ipynb` and `how_to_use.ipynb` can be used to visualize the model's behavior and better understand its execution.


## Contribution

You can see more in https://github.com/Jackson-Kang/Pytorch-Diffusion-Model-Tutorial. Feel free to contribute improvements to the code or documentation. To do so, fork the repository, create a branch, and submit a pull request.

